{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\egora\\ML\\nlp_related\\article-summarizer\\venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import openai\n",
    "import langchain\n",
    "import pinecone\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "environment = os.getenv('PINECONE_ENV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the paper\n",
    "\n",
    "Let's create a function which with help of `ArxivLoader` will load a paper based on its id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_loader(paper_id: str) -> list[Document]:\n",
    "    docs = ArxivLoader(query=paper_id, load_max_docs=2).load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "doc = arxiv_loader(paper_id='2402.17764')\n",
    "print(len(doc), type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting document into chunks\n",
    "\n",
    "Since the number of tokens for the LLM is limited we need to separate our document into chunks with a bit of overlap. Fir this purpose `RecursiveCharacterTextSplitter` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50) -> list:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = chunk_data(docs=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here how `document` looks like after being separated into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Embeddings\n",
    "\n",
    "In order to load the document into vector search database (index) we need to convert our text into embeddings. I will use `openai` tools for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test vectors with that embeddings and just convert any sentence into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = embeddings.embed_query(\"Good morning, how's your day?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0023635016767517016,\n",
       " 0.0046612636186404545,\n",
       " 0.003153944514078891,\n",
       " -0.034961042460283254,\n",
       " -0.012897519910202862,\n",
       " 0.046255765468743076,\n",
       " -0.019033233464465366,\n",
       " -0.003163335738465989,\n",
       " -0.028549849265375436,\n",
       " -0.011476288076927634,\n",
       " 0.011470026795008262,\n",
       " 0.010092622072523406,\n",
       " -0.009554181629781073,\n",
       " -0.014763277614042917,\n",
       " 0.003274467438937849,\n",
       " 0.005697448034294434,\n",
       " 0.02947646683537609,\n",
       " -0.015990419952366372,\n",
       " 0.027422881849826244,\n",
       " -0.011538897170830896,\n",
       " -0.003909951697833458,\n",
       " 0.0042605639275433895,\n",
       " -0.004423348409882225,\n",
       " -0.0035061218314380127,\n",
       " 0.014838408340462312,\n",
       " 0.007369116708844023,\n",
       " 0.006736762625246794,\n",
       " -0.010436973020313968,\n",
       " 0.01015523116642667,\n",
       " -0.020936555134531198,\n",
       " 0.010217840260329934,\n",
       " -0.01450031774326851,\n",
       " -0.020360550725563087,\n",
       " 0.007594510285086122,\n",
       " 0.0020066286307837047,\n",
       " -0.02028541906782108,\n",
       " -0.0012060120252448225,\n",
       " -0.010643584147781871,\n",
       " 0.016829385163431507,\n",
       " -0.017668350374496638,\n",
       " 0.018256877347303496,\n",
       " 0.0067242409927306636,\n",
       " 0.011513853905798637,\n",
       " -0.020197766708885557,\n",
       " -0.031780490059184215,\n",
       " 0.0010643584380612525,\n",
       " 0.01078132489942714,\n",
       " -0.01774348016959342,\n",
       " -0.01676677606952824,\n",
       " 0.03218118975028128,\n",
       " 0.021262125030531483,\n",
       " 0.00209584700869103,\n",
       " -0.023478494032758857,\n",
       " -0.01254690768049293,\n",
       " -0.0001363709216024169,\n",
       " 0.002062977141259556,\n",
       " -0.025494515052082924,\n",
       " 0.02013515761498229,\n",
       " -0.01145124388057276,\n",
       " -0.011369851872233995,\n",
       " -0.008414692581715753,\n",
       " -0.0066991972620370965,\n",
       " -0.00100566217969629,\n",
       " 0.019696892095014226,\n",
       " -0.0010776628472326307,\n",
       " 0.013173000482170788,\n",
       " -0.0064612815876175615,\n",
       " -0.0005576142216364902,\n",
       " -0.025920258008212252,\n",
       " 0.005246660881810236,\n",
       " 0.019609237873433476,\n",
       " -0.0008820087885006367,\n",
       " 0.001224012163025076,\n",
       " -0.016353553814592435,\n",
       " -0.0015847983937773535,\n",
       " 0.0011825334986157723,\n",
       " -0.026947050500987176,\n",
       " 0.014725711785171914,\n",
       " -0.013961878231848787,\n",
       " -0.012803605338025355,\n",
       " 0.016666601146753977,\n",
       " -0.04057083813564216,\n",
       " -0.02108681845001521,\n",
       " 0.025845126350470246,\n",
       " 0.006029277349568865,\n",
       " 0.0017921917158238725,\n",
       " -0.014274924632687718,\n",
       " 0.003966300441139962,\n",
       " -0.014512840307107252,\n",
       " -0.016979646616270293,\n",
       " -0.00571310030777025,\n",
       " 0.02324057835833932,\n",
       " 0.03060343425092528,\n",
       " 0.025870169615502505,\n",
       " -0.0324065847235073,\n",
       " 0.004914831566535806,\n",
       " 0.01719251902565757,\n",
       " 0.017781045998464425,\n",
       " -0.01188950940054083,\n",
       " -0.027022182158729178,\n",
       " 0.0033120330349781985,\n",
       " -0.009059568297829113,\n",
       " -0.010549670506926977,\n",
       " -0.005224747792076355,\n",
       " -0.01563980679133383,\n",
       " -7.204962745708315e-05,\n",
       " -0.008784087725861187,\n",
       " -0.025870169615502505,\n",
       " 0.026746700655438643,\n",
       " -0.013285697037461184,\n",
       " -0.02494355204550185,\n",
       " 0.01865757703840056,\n",
       " 0.0018454097017553647,\n",
       " -0.017442955401270624,\n",
       " -0.0011363591055975931,\n",
       " -0.0024527200546590276,\n",
       " -0.005334313706407066,\n",
       " 0.0005313965587907048,\n",
       " -0.004091519094607795,\n",
       " -0.0019440193040497881,\n",
       " 0.021762999644402814,\n",
       " 0.005688056577076684,\n",
       " 0.01813165915949697,\n",
       " -0.028950548956472498,\n",
       " 0.029050723879246766,\n",
       " -0.004251172470325639,\n",
       " -0.0064362378569239945,\n",
       " -0.006642848518730592,\n",
       " 0.0007943557310731538,\n",
       " 0.011100631650862829,\n",
       " 0.014387621187978114,\n",
       " 0.01754313032404489,\n",
       " 0.0025200251100018192,\n",
       " -0.011432461897459872,\n",
       " -0.03488591080254125,\n",
       " 0.0034779476926154137,\n",
       " -0.013986921496881048,\n",
       " 0.02107429588617647,\n",
       " -0.02301518524775853,\n",
       " -0.011720464101943927,\n",
       " 0.008583737880312654,\n",
       " 0.02654635080989011,\n",
       " 0.018908013414013613,\n",
       " -0.0035906444807364633,\n",
       " -0.02945142357034383,\n",
       " 0.01973445792388523,\n",
       " 0.01694208078739929,\n",
       " -0.023566148254339607,\n",
       " 0.006285975472762596,\n",
       " -0.02266457394937121,\n",
       " 0.024254850149920727,\n",
       " -0.004880396378624488,\n",
       " 0.01796887514281944,\n",
       " 0.033433375353636996,\n",
       " 0.004003865804349659,\n",
       " 0.03238153959582981,\n",
       " -0.01930871310511068,\n",
       " 0.01998489429949828,\n",
       " -0.014437708649365246,\n",
       " -0.011689159554992296,\n",
       " -0.0024448939179211194,\n",
       " -0.0007407464732293641,\n",
       " 0.01592781085846311,\n",
       " 0.0038285596894946927,\n",
       " -0.017167475760625307,\n",
       " 0.027673318225439298,\n",
       " 0.012609516774396194,\n",
       " 0.019897241940562756,\n",
       " 0.001399318277880042,\n",
       " -0.009422702160055176,\n",
       " 0.00916600450252275,\n",
       " -0.009140960306167878,\n",
       " -0.038667516465576326,\n",
       " 0.029100810409311288,\n",
       " 0.0006339193458694088,\n",
       " 0.02434250250885625,\n",
       " -0.0033245549003249816,\n",
       " -0.014124662248526317,\n",
       " -0.023090316905500535,\n",
       " -0.004514131875438741,\n",
       " -0.0395690889078995,\n",
       " 0.004855352647930922,\n",
       " 0.014412665384332985,\n",
       " 0.021374820654499267,\n",
       " -0.0019862806286990134,\n",
       " -0.008358343838409249,\n",
       " 0.01254690768049293,\n",
       " -0.0038316903304543783,\n",
       " 0.03220623487795876,\n",
       " -0.01369891836107438,\n",
       " -0.003678297538163946,\n",
       " 0.02106177518498295,\n",
       " 0.010236623174765435,\n",
       " -0.018269399911142237,\n",
       " -0.6547432207467953,\n",
       " -0.030052473106989427,\n",
       " 0.007099896487472856,\n",
       " 0.008521127855086779,\n",
       " 0.0061826703746899496,\n",
       " 0.014587971033526647,\n",
       " -0.007137462316343859,\n",
       " 0.02218874260053214,\n",
       " -0.03002742984195717,\n",
       " 0.03924352087454444,\n",
       " 0.0041009105518255455,\n",
       " 0.00854617205144165,\n",
       " 0.02013515761498229,\n",
       " 0.0014807104026341334,\n",
       " -0.0043826524057128426,\n",
       " -0.026295914434277055,\n",
       " -0.008953132093135476,\n",
       " -0.008571215316473911,\n",
       " -0.016603992052850715,\n",
       " 0.015739981714108096,\n",
       " -0.02268961721440347,\n",
       " 0.0021068035535579707,\n",
       " -0.019333758232788163,\n",
       " 0.0033840335860992124,\n",
       " 0.002638982714380934,\n",
       " -0.005616056025955669,\n",
       " 0.0058633626919316496,\n",
       " 0.000299546443022897,\n",
       " -0.031780490059184215,\n",
       " 0.03638853650680046,\n",
       " -0.01929619240391716,\n",
       " 0.014863452536817184,\n",
       " 0.022489267368854936,\n",
       " -0.008020253241215447,\n",
       " 0.06035538258959191,\n",
       " -0.023566148254339607,\n",
       " -0.0222137858655644,\n",
       " 0.025569644847179707,\n",
       " -0.01642868547233444,\n",
       " 0.03573740044009034,\n",
       " -0.018219311518432493,\n",
       " 0.002206978476332237,\n",
       " 0.015314238757978771,\n",
       " -0.00807034070260258,\n",
       " -0.003637601301163911,\n",
       " -0.020873946040627936,\n",
       " 0.022652051385532466,\n",
       " -0.008064080352005822,\n",
       " 0.021474995577273535,\n",
       " -0.012133685425557122,\n",
       " 0.02480581129385658,\n",
       " 0.026095564588728522,\n",
       " -0.0073190292474568895,\n",
       " 0.00659902233926283,\n",
       " 0.002056716092170838,\n",
       " 0.014011965693235921,\n",
       " 0.028449674342601167,\n",
       " -0.004363869491277341,\n",
       " -0.003650123166510694,\n",
       " -0.0025028075160461604,\n",
       " -0.0161156381401729,\n",
       " 0.011156980394169333,\n",
       " -0.009278701057813147,\n",
       " -0.012265165826605633,\n",
       " -0.013749005822461514,\n",
       " 0.002609243255078492,\n",
       " 0.006010494435133363,\n",
       " 0.0013140130847569967,\n",
       " 0.009403919245619674,\n",
       " -0.02787366807098783,\n",
       " 0.013335784498848318,\n",
       " 0.017392868871206103,\n",
       " -0.003496730374220262,\n",
       " 0.014400142820494243,\n",
       " 0.005910319512359097,\n",
       " 0.02739783672214876,\n",
       " 0.03473565121234768,\n",
       " 0.005853971234713899,\n",
       " 0.0018798447732513556,\n",
       " 0.023578668955533125,\n",
       " 0.007844947592021787,\n",
       " 0.01269090878273496,\n",
       " -0.02316544856324254,\n",
       " -0.0036845583544220114,\n",
       " 0.0221762200366934,\n",
       " -0.018620011209529556,\n",
       " -0.05229130223758609,\n",
       " -0.00837712675284475,\n",
       " 0.0035311655621315793,\n",
       " 0.0056285776584718,\n",
       " 0.005972929071923666,\n",
       " 0.012596995141880063,\n",
       " 0.01435005535910711,\n",
       " -0.0054970981887459015,\n",
       " 0.009447746356410047,\n",
       " 0.027498011644923027,\n",
       " -0.025795039820405725,\n",
       " 0.014800842511591309,\n",
       " -0.0020645422289087456,\n",
       " -0.022639528821693726,\n",
       " -0.00151123240576117,\n",
       " -0.005631708299431485,\n",
       " 0.016403642207302182,\n",
       " 0.01369891836107438,\n",
       " 0.03235649446815233,\n",
       " -0.007162506047037425,\n",
       " -0.013147957217138528,\n",
       " 0.03438503805131514,\n",
       " 0.009961142602797509,\n",
       " -0.022339004053370925,\n",
       " 0.01448779611075238,\n",
       " 0.013072825559396522,\n",
       " -0.021212036637821736,\n",
       " 0.007963905429231555,\n",
       " 0.01592781085846311,\n",
       " -0.020548378007272876,\n",
       " -0.015602241893785439,\n",
       " 0.017730959468399903,\n",
       " 0.020936555134531198,\n",
       " -0.009710705295861844,\n",
       " 0.005331183531108686,\n",
       " -0.005303009159455434,\n",
       " 0.03002742984195717,\n",
       " 0.0033433375819298303,\n",
       " 0.004764569182374406,\n",
       " 0.02028541906782108,\n",
       " -0.04881022320551903,\n",
       " 0.0003780037665002729,\n",
       " -0.01416222807739732,\n",
       " -0.005891537063584902,\n",
       " 0.017205039726851088,\n",
       " 0.00011377287319136239,\n",
       " 0.0010424451154967184,\n",
       " -0.025131381189856864,\n",
       " 0.013398394524074193,\n",
       " -0.02429241597879173,\n",
       " 0.01482588670794618,\n",
       " 0.008915567195587084,\n",
       " 0.026020432930986517,\n",
       " -0.017756002733432162,\n",
       " -0.014475274478236249,\n",
       " -0.014137183881042448,\n",
       " 0.0270722686887937,\n",
       " 0.022088567677757874,\n",
       " 0.011520114256395396,\n",
       " -0.0006984852067878022,\n",
       " -0.006264062383028715,\n",
       " 0.0012521864182630017,\n",
       " -0.014600492666042776,\n",
       " 0.033483461883701514,\n",
       " -0.010249145738604177,\n",
       " -0.006098147725391499,\n",
       " -0.005760057128197698,\n",
       " 0.017643305246819153,\n",
       " 0.0034215991821395625,\n",
       " -0.008239386001199481,\n",
       " -0.014174749709913451,\n",
       " -0.03060343425092528,\n",
       " -0.0063767589383191105,\n",
       " -0.0241296319621142,\n",
       " -0.006132582913302816,\n",
       " -0.005027528121826202,\n",
       " -0.019721935360046485,\n",
       " -0.0034435125047040964,\n",
       " 0.00201132435939258,\n",
       " -0.029827079996408636,\n",
       " -0.01126341659886297,\n",
       " 0.038842821183447375,\n",
       " -0.022251351694435404,\n",
       " -0.02849976087266569,\n",
       " -0.0012717517601077714,\n",
       " -0.031204483787570878,\n",
       " -0.0183320090050455,\n",
       " -0.012371601099976658,\n",
       " 0.002892550429445632,\n",
       " -0.003556209292825146,\n",
       " -0.0011207068321217773,\n",
       " -0.024768245464985576,\n",
       " 0.004871004921406737,\n",
       " -0.013298219601299927,\n",
       " 0.0008655737965772363,\n",
       " -0.006799371719150057,\n",
       " -0.013210566311041791,\n",
       " 0.005321792073890936,\n",
       " 0.022564399026596942,\n",
       " 0.006220235737899646,\n",
       " 0.01801896167288396,\n",
       " 0.018156702424529228,\n",
       " -0.015351804586849774,\n",
       " 0.02061098710117614,\n",
       " 0.009754531475329606,\n",
       " 0.02365380061327513,\n",
       " -0.023991891210468932,\n",
       " -0.014750755050204175,\n",
       " 0.00015505588709260166,\n",
       " -0.012240121630250761,\n",
       " -0.002354110452364604,\n",
       " 0.03405947001796008,\n",
       " 0.028549849265375436,\n",
       " 0.0023713278134896098,\n",
       " 0.04184806886667536,\n",
       " 0.015802592670656583,\n",
       " 0.001987845949178856,\n",
       " -0.013586221805783984,\n",
       " -0.005021267305568137,\n",
       " -0.011369851872233995,\n",
       " 0.01895810180672336,\n",
       " -0.019083319994529887,\n",
       " 0.005879014965407465,\n",
       " 0.0157775475429791,\n",
       " -0.004880396378624488,\n",
       " -0.0008193994617667203,\n",
       " 0.020961600262208682,\n",
       " 0.0013492308164929087,\n",
       " -0.00838338803476412,\n",
       " 0.017781045998464425,\n",
       " 0.003352729039147581,\n",
       " -0.0018532358384932726,\n",
       " -0.016190769797914905,\n",
       " 0.008433475496151255,\n",
       " 0.005133963860858534,\n",
       " 0.03260693456905583,\n",
       " 0.004066475363914228,\n",
       " -0.02047324634953087,\n",
       " -0.011232112051911339,\n",
       " 0.025820083085437984,\n",
       " 0.026320957699309314,\n",
       " 0.03228136281041032,\n",
       " -0.0009461833192990694,\n",
       " -0.026646525732664374,\n",
       " -0.01736782560617384,\n",
       " 0.02950151010040835,\n",
       " -0.005346835804584503,\n",
       " -0.0017984526484972642,\n",
       " 0.012114903442444234,\n",
       " 0.006455020771359496,\n",
       " -0.010762541984991639,\n",
       " 0.0024214152748767426,\n",
       " 0.0434007811009991,\n",
       " -0.009084612494183986,\n",
       " -0.016967125915076774,\n",
       " 0.007813643045070156,\n",
       " 0.03536174215138031,\n",
       " -0.035987836815703396,\n",
       " 0.0029191594806190413,\n",
       " 0.007757294301763652,\n",
       " 0.014412665384332985,\n",
       " 0.007832425959505658,\n",
       " 0.006029277349568865,\n",
       " 0.019196017481142892,\n",
       " -0.005400053906931321,\n",
       " -0.012628299688831696,\n",
       " -0.017330259777302837,\n",
       " 0.003934995428527024,\n",
       " 0.025244076813824647,\n",
       " -0.02163777959395106,\n",
       " -0.005171529689729536,\n",
       " -0.010837672711411032,\n",
       " -0.0019643673061344796,\n",
       " 0.018557402115626294,\n",
       " 0.021424909047209013,\n",
       " 0.017943830015141955,\n",
       " 0.013260653772428924,\n",
       " -0.002397936864663019,\n",
       " 0.013561178540751723,\n",
       " 0.00322124933659103,\n",
       " 0.0041572588294707434,\n",
       " -0.00981714150055548,\n",
       " -0.008082863266441323,\n",
       " 0.01435005535910711,\n",
       " -0.015351804586849774,\n",
       " -0.005152746775294035,\n",
       " 0.011194546223040336,\n",
       " -0.011219589488072597,\n",
       " 0.01702973500898004,\n",
       " -0.002003498222654672,\n",
       " 0.002563851522300234,\n",
       " -0.015339282954333643,\n",
       " -0.0018798447732513556,\n",
       " 0.011707942469427798,\n",
       " -0.023804062066113917,\n",
       " -0.04725751469648574,\n",
       " -0.0007094418098624059,\n",
       " 0.010061317525571776,\n",
       " 0.00716876686329549,\n",
       " 0.005844579777496148,\n",
       " -0.028624980923117438,\n",
       " -0.00318211865290149,\n",
       " -0.005227877967374735,\n",
       " 0.00330577198588948,\n",
       " -0.0013453177481239549,\n",
       " 0.03183057658924874,\n",
       " -0.017455477965109364,\n",
       " -0.030478216063118752,\n",
       " 0.002264892074457278,\n",
       " -0.00950409416839394,\n",
       " 0.0352114788358963,\n",
       " 0.013874224941590652,\n",
       " 0.013999443129397179,\n",
       " -0.0013961877533356828,\n",
       " 0.0062390186523351475,\n",
       " -0.010718714874201266,\n",
       " 0.002967681621526332,\n",
       " -0.024655549841017792,\n",
       " -0.002967681621526332,\n",
       " -0.016053029046269637,\n",
       " -0.00467691589211627,\n",
       " -0.031429876898151674,\n",
       " -3.619601338006647e-05,\n",
       " -0.007488074546053791,\n",
       " 0.0016278422622511737,\n",
       " 0.023378319109984592,\n",
       " -0.01353613434439685,\n",
       " -0.018933058541691097,\n",
       " 0.026972093766019434,\n",
       " 0.010123926619475039,\n",
       " -0.00838964838536088,\n",
       " -0.017768525297270906,\n",
       " 0.034084511420347116,\n",
       " -0.012039771784702228,\n",
       " -0.013874224941590652,\n",
       " -0.010743759070556138,\n",
       " -0.00788877377148955,\n",
       " 0.0011277503086197637,\n",
       " 0.10307997877091296,\n",
       " 0.030778740831441553,\n",
       " 0.001303839083714651,\n",
       " 0.02900063548653702,\n",
       " 0.00838338803476412,\n",
       " 0.019571672044562474,\n",
       " 0.015602241893785439,\n",
       " -0.03060343425092528,\n",
       " 0.009203569400071141,\n",
       " -0.004489088144745174,\n",
       " 0.007769816399941088,\n",
       " -0.032807284414604364,\n",
       " -0.02385415045882366,\n",
       " -0.0037847332771962774,\n",
       " -0.009554181629781073,\n",
       " 0.008940610460619345,\n",
       " -0.003650123166510694,\n",
       " -0.019120885823400886,\n",
       " -0.016516337831269965,\n",
       " -0.0120648159810571,\n",
       " -0.0018422791772110057,\n",
       " 0.02173795451672533,\n",
       " -0.0023728931339694523,\n",
       " 0.029676816680924624,\n",
       " 0.016153203969043902,\n",
       " 0.019847153547853012,\n",
       " 0.006073103994697932,\n",
       " -0.011006718010007934,\n",
       " -0.0004018735650416206,\n",
       " -0.008640085692296547,\n",
       " -0.009792097304200609,\n",
       " -0.0005173094893794052,\n",
       " 0.003703341268857513,\n",
       " -0.024054500304372194,\n",
       " -0.0076633806609087566,\n",
       " 0.02769836149047156,\n",
       " 0.0032243799775507156,\n",
       " 0.025644776504921713,\n",
       " 0.010825151078894903,\n",
       " 0.013749005822461514,\n",
       " 0.024304936679985248,\n",
       " 0.023440928203887854,\n",
       " 0.002385414999316236,\n",
       " -0.016566426223979712,\n",
       " 0.027422881849826244,\n",
       " -0.00788251342089279,\n",
       " -0.022915010324984265,\n",
       " -0.003969430616438342,\n",
       " -0.011563941367185769,\n",
       " -0.01255316803108969,\n",
       " 0.013886746574106783,\n",
       " 0.016854428428463766,\n",
       " -0.00966061783447471,\n",
       " -0.007143723132601924,\n",
       " 0.0059791898881817315,\n",
       " 0.004745786267938905,\n",
       " -0.020999166091079685,\n",
       " -0.009635573638119838,\n",
       " 0.0027798536413245825,\n",
       " -0.005365618719020003,\n",
       " 0.008959393375054846,\n",
       " 0.003775341819978527,\n",
       " -0.004808395827503474,\n",
       " 0.0015221890670434372,\n",
       " -0.0013014912194102134,\n",
       " -0.04089640616899722,\n",
       " -0.005340574988326437,\n",
       " 0.009867228961942615,\n",
       " -0.010906543087233668,\n",
       " 0.018920535977852357,\n",
       " -0.05735013490636392,\n",
       " -0.032306407938087804,\n",
       " -0.029827079996408636,\n",
       " 0.022977619418887527,\n",
       " 0.04600532536783958,\n",
       " 0.016228335626785908,\n",
       " 0.013010216465493258,\n",
       " -0.008095384898957452,\n",
       " 0.012872475713847989,\n",
       " 0.013298219601299927,\n",
       " 0.011789334477766563,\n",
       " -0.028074017916536364,\n",
       " 0.01738034630736736,\n",
       " 0.0036970802197687945,\n",
       " -0.009604269091168205,\n",
       " -0.008402170949199622,\n",
       " 0.012791083705509224,\n",
       " -0.008107906531473584,\n",
       " 0.0026186347122962427,\n",
       " 0.024855899686566325,\n",
       " 0.025356774300437656,\n",
       " 0.03901812590131842,\n",
       " 0.02108681845001521,\n",
       " -0.008896784281151583,\n",
       " -0.006173278917472199,\n",
       " -0.0008389648618191534,\n",
       " -0.0038003855506720933,\n",
       " 0.007037287393569592,\n",
       " -0.0025403731120865105,\n",
       " -0.0034497733209621615,\n",
       " 0.011488809709443763,\n",
       " -0.01382413748020352,\n",
       " 0.009416441809458416,\n",
       " -0.03373390198460502,\n",
       " -0.004078996996430358,\n",
       " 0.01644120617352796,\n",
       " 0.0020817598228644045,\n",
       " 0.018745229397336083,\n",
       " -0.01978454445394975,\n",
       " 0.007494335362311855,\n",
       " 0.028549849265375436,\n",
       " -0.02494355204550185,\n",
       " 0.027623231695374777,\n",
       " -0.006786850086633926,\n",
       " 0.015414413680753037,\n",
       " 0.03473565121234768,\n",
       " 0.026621482467632115,\n",
       " 0.027723406618149045,\n",
       " -0.001289751897888025,\n",
       " -0.022263874258274145,\n",
       " 0.014976149092107581,\n",
       " -0.03200588503241023,\n",
       " 0.026671568997696637,\n",
       " 0.001449405739267175,\n",
       " 0.00473952545168084,\n",
       " 0.006630326886214462,\n",
       " 0.0074004212557956545,\n",
       " -0.021487518141112275,\n",
       " -0.02170039055049955,\n",
       " -0.00611066935790763,\n",
       " -0.0029598554847884238,\n",
       " 0.014137183881042448,\n",
       " -0.017518087059012626,\n",
       " -0.03746541739228513,\n",
       " -0.011313504060250104,\n",
       " -0.04182302373899788,\n",
       " -0.006204583464423831,\n",
       " 0.024818333857695322,\n",
       " -0.0006683544755730628,\n",
       " 0.0005055702260648801,\n",
       " -0.028775242375956227,\n",
       " -0.012791083705509224,\n",
       " -0.013285697037461184,\n",
       " -0.025669819769953972,\n",
       " -0.03150500855589368,\n",
       " -0.03688940925802657,\n",
       " 0.0025075032446550357,\n",
       " 0.011845683221073067,\n",
       " -0.021525083969983278,\n",
       " 0.009854706398103872,\n",
       " -0.007494335362311855,\n",
       " 0.029601685023182618,\n",
       " -0.03443512458137966,\n",
       " -0.0049179617418341854,\n",
       " -0.010230362824168676,\n",
       " -0.036964540915768576,\n",
       " 0.007763555583683022,\n",
       " 0.005312400616673185,\n",
       " 0.02975194833866663,\n",
       " 0.01995985103446602,\n",
       " 0.03107926559976435,\n",
       " 0.002235152615154836,\n",
       " 0.028349499419826903,\n",
       " -0.003249423708244282,\n",
       " -0.022151176771661136,\n",
       " -0.028524806000343173,\n",
       " -0.0025247208386106946,\n",
       " -0.018306963877368018,\n",
       " -0.01498867072462371,\n",
       " -0.003274467438937849,\n",
       " 0.019033233464465366,\n",
       " 0.015176498937656113,\n",
       " 0.018457227192852026,\n",
       " 0.004248042295027259,\n",
       " -0.015502066971011173,\n",
       " 0.004864744105148673,\n",
       " 0.003737776223938177,\n",
       " -0.011995945605234466,\n",
       " -0.04214859177235294,\n",
       " -0.021111861715047472,\n",
       " 0.011607767546653531,\n",
       " 0.0011355765617729982,\n",
       " -0.02235152661720967,\n",
       " 0.010868977258362665,\n",
       " 0.00659902233926283,\n",
       " -0.029276116989827558,\n",
       " 0.016629035317882974,\n",
       " 0.018344529706239017,\n",
       " 0.030878915754215818,\n",
       " -0.01222133871581526,\n",
       " 0.024993640438211593,\n",
       " -0.002293066213279877,\n",
       " 0.005359357437100633,\n",
       " -0.003274467438937849,\n",
       " 0.017142430632947823,\n",
       " 0.007293985516763323,\n",
       " -0.022652051385532466,\n",
       " -0.0011942727037226341,\n",
       " 0.01432501209407485,\n",
       " 0.020523334742240617,\n",
       " 0.014087096419655314,\n",
       " 0.02047324634953087,\n",
       " 0.008878001366716081,\n",
       " 0.012897519910202862,\n",
       " 0.010424451387797837,\n",
       " 0.017017212445141296,\n",
       " -0.007212593508424558,\n",
       " -0.003462295186308945,\n",
       " 0.018995667635594363,\n",
       " -0.004968049203221319,\n",
       " 0.010637322865862501,\n",
       " -0.03027786621757022,\n",
       " -0.02732270692705198,\n",
       " -0.020873946040627936,\n",
       " 0.024405111602759516,\n",
       " -0.0054376192701410175,\n",
       " -0.02026037580278882,\n",
       " 0.025043726968276114,\n",
       " -0.031229527052603137,\n",
       " -0.015063802382365717,\n",
       " -0.033082762192604455,\n",
       " 0.0053092699757135,\n",
       " 0.021237079902854,\n",
       " 0.004645611345164638,\n",
       " 0.02709731195382596,\n",
       " -0.008477301675619017,\n",
       " -0.006617804788037026,\n",
       " -0.00286281097014319,\n",
       " 6.520173598353998e-05,\n",
       " 0.009748271124732847,\n",
       " -0.0006112234794802798,\n",
       " 0.01838209553511002,\n",
       " 0.010631062515265742,\n",
       " -4.942712091353108e-05,\n",
       " 0.0007063113435257101,\n",
       " 0.0018203659710617982,\n",
       " 0.01382413748020352,\n",
       " -0.011207067855556465,\n",
       " -0.002928550937836792,\n",
       " 0.02604547619601878,\n",
       " 0.020711162023950406,\n",
       " -0.01528919549294651,\n",
       " -0.012365340749379899,\n",
       " 0.005174660330689222,\n",
       " -0.008320778009538246,\n",
       " 0.018745229397336083,\n",
       " 0.003847342603930194,\n",
       " -0.0026828093595100016,\n",
       " -0.018995667635594363,\n",
       " 0.006085625627214063,\n",
       " -0.027472968379890765,\n",
       " 0.018419661363981023,\n",
       " -0.0176933936395289,\n",
       " 0.016165726532882646,\n",
       " 0.016040506482430893,\n",
       " 0.004620567614471072,\n",
       " 0.012127425074960363,\n",
       " 0.001682625452247182,\n",
       " 0.004877265737664803,\n",
       " 0.01575250427794684,\n",
       " -0.0049210923827938706,\n",
       " 0.016641556019076492,\n",
       " -0.021349777389467008,\n",
       " 0.017442955401270624,\n",
       " -0.005525272094737848,\n",
       " -0.0016888863849205736,\n",
       " -0.010418190105878467,\n",
       " -0.005969798430963981,\n",
       " 0.0043826524057128426,\n",
       " 0.02870011071821422,\n",
       " -0.005894667238883281,\n",
       " -0.013949355668010045,\n",
       " -0.013147957217138528,\n",
       " 0.024981117874372852,\n",
       " 0.0170547782740123,\n",
       " 0.02496859531053411,\n",
       " -0.028449674342601167,\n",
       " 0.02113690498007973,\n",
       " -0.014525361939623383,\n",
       " -0.0032087274712442465,\n",
       " 0.0292510737247953,\n",
       " -0.015339282954333643,\n",
       " -0.007913817967844423,\n",
       " -0.0002731331442209629,\n",
       " -0.0036876887625510437,\n",
       " -0.017468000528948105,\n",
       " -0.036288359721380975,\n",
       " -0.013736484189945383,\n",
       " -0.00249185097117922,\n",
       " -0.022652051385532466,\n",
       " -0.01175176864889556,\n",
       " -0.013949355668010045,\n",
       " 0.0033621204963653318,\n",
       " 0.010092622072523406,\n",
       " 0.015101367279914108,\n",
       " -0.024430156730437,\n",
       " 0.007431726268408592,\n",
       " -0.019897241940562756,\n",
       " 0.002424545915836428,\n",
       " 0.016541381096302227,\n",
       " 0.0013828833441643048,\n",
       " 0.0009422702509301155,\n",
       " -0.017893743485077433,\n",
       " -0.00627971465650453,\n",
       " -0.010518365028652733,\n",
       " -0.0267717439204709,\n",
       " -0.0037409068648978625,\n",
       " -0.013749005822461514,\n",
       " -0.015339282954333643,\n",
       " -0.014901017434365576,\n",
       " 0.005631708299431485,\n",
       " 0.014299967897719978,\n",
       " -0.00950409416839394,\n",
       " -0.02754810003763277,\n",
       " 0.008953132093135476,\n",
       " 0.010061317525571776,\n",
       " 0.0028753328354899736,\n",
       " 0.00249185097117922,\n",
       " -0.017330259777302837,\n",
       " 0.031029177207054608,\n",
       " -0.03741532713693016,\n",
       " -0.0012920997621924627,\n",
       " 0.02897559222150476,\n",
       " -0.010862716907765906,\n",
       " 0.015852679200721104,\n",
       " 0.003916212979752829,\n",
       " 0.0071562452307793595,\n",
       " -0.008289473462586615,\n",
       " -0.01546450114214017,\n",
       " 0.010800107813862642,\n",
       " -0.0015002757444789031,\n",
       " 0.005162138232511786,\n",
       " -0.00916600450252275,\n",
       " 0.000983748857131756,\n",
       " -0.003803516191631779,\n",
       " 0.003452903961921847,\n",
       " -0.02172543381553181,\n",
       " 0.023140403435565057,\n",
       " 0.0022085437968120797,\n",
       " -0.008746521896990184,\n",
       " 0.0372650675467366,\n",
       " 0.021212036637821736,\n",
       " -0.002638982714380934,\n",
       " -0.003722123950462361,\n",
       " 0.008283213111989856,\n",
       " -0.02560721067605071,\n",
       " -0.015827635935688845,\n",
       " -0.01692956008620577,\n",
       " -0.01208359796416999,\n",
       " -0.03561217852699337,\n",
       " -0.0020081939512635472,\n",
       " 0.01740538957239962,\n",
       " 0.007938861232876683,\n",
       " -0.008671390239248178,\n",
       " -0.019834632846659494,\n",
       " -0.010267927721717068,\n",
       " -0.022326483352177406,\n",
       " -0.013235609576074052,\n",
       " 0.013736484189945383,\n",
       " 0.02704722542376144,\n",
       " 0.004986832117656819,\n",
       " -0.0008968784599441944,\n",
       " -0.011995945605234466,\n",
       " 0.02945142357034383,\n",
       " 0.00604492962304468,\n",
       " -0.009003219554522608,\n",
       " 0.007419204170231156,\n",
       " 0.018419661363981023,\n",
       " -0.024254850149920727,\n",
       " -0.014663102691268652,\n",
       " 0.02047324634953087,\n",
       " -0.013861703309074522,\n",
       " -0.030327954610279966,\n",
       " -0.003337076765671765,\n",
       " -0.00169045158898509,\n",
       " -0.03849220802241483,\n",
       " 0.001939323575440913,\n",
       " 0.0040383009922609755,\n",
       " -0.00013695788767852633,\n",
       " 0.003731515407680112,\n",
       " -0.000750920532479373,\n",
       " -0.011870726486105328,\n",
       " 0.0001913497330026583,\n",
       " 0.011125675847217702,\n",
       " 0.013273175404945055,\n",
       " -0.014613015229881518,\n",
       " -0.0057819706835928845,\n",
       " 0.01869514286727156,\n",
       " -0.015163977305139983,\n",
       " -0.016666601146753977,\n",
       " 0.006567717326649893,\n",
       " 0.020335505597885602,\n",
       " -0.007112418585650292,\n",
       " 0.01672921024065724,\n",
       " -0.042373986745578955,\n",
       " -0.0076383369302151895,\n",
       " 0.016553903660140968,\n",
       " 0.00667415353134353,\n",
       " 0.014525361939623383,\n",
       " -0.0024527200546590276,\n",
       " 0.0032243799775507156,\n",
       " 0.024392590901565998,\n",
       " 0.013473525250493588,\n",
       " -0.012935084807751253,\n",
       " -0.01627842215685043,\n",
       " 0.017280171384593094,\n",
       " -0.024154675227146462,\n",
       " -0.011495070991363135,\n",
       " 0.0065426735959563265,\n",
       " -0.013961878231848787,\n",
       " -0.0019408888959207555,\n",
       " -0.00290037656618354,\n",
       " -0.0011653159046601136,\n",
       " -0.025744951427695978,\n",
       " -0.025031206267082596,\n",
       " -0.025331731035405397,\n",
       " 0.0012255773670895923,\n",
       " -0.0322312762803458,\n",
       " 0.0025967213897317085,\n",
       " 0.005149616599995656,\n",
       " -0.02699713889369692,\n",
       " 0.003878647150881826,\n",
       " -0.007463030815360224,\n",
       " -0.010731237438040006,\n",
       " 0.018056527501754963,\n",
       " 0.0008553997955348906,\n",
       " 0.013711440924913122,\n",
       " -0.017392868871206103,\n",
       " 0.01914592908843315,\n",
       " 0.015714938449075837,\n",
       " -0.007920078318441182,\n",
       " -0.016691644411786236,\n",
       " 0.02300266268391979,\n",
       " 0.002266457394937121,\n",
       " -0.014863452536817184,\n",
       " 0.030528302593183274,\n",
       " 0.23561138856275063,\n",
       " 0.002684374447159192,\n",
       " -0.007256420153553626,\n",
       " 0.039894656941254554,\n",
       " 0.01319804467852566,\n",
       " 0.02493103134430833,\n",
       " 0.017280171384593094,\n",
       " 0.0024167197790985204,\n",
       " -0.00838964838536088,\n",
       " 0.02106177518498295,\n",
       " -0.0063798895792787965,\n",
       " -0.014287446265203847,\n",
       " -0.026671568997696637,\n",
       " 0.0035249045130428614,\n",
       " 0.016027985781237375,\n",
       " 0.004608045981954942,\n",
       " -0.025995389665954258,\n",
       " -0.029200985332085552,\n",
       " 0.01093158728358854,\n",
       " -0.024380068337727254,\n",
       " -0.004172911102946559,\n",
       " -0.008671390239248178,\n",
       " -0.014550405204655644,\n",
       " 0.0007481813380549745,\n",
       " 0.020272896503982337,\n",
       " 0.03909325755906043,\n",
       " 0.0009994012470228983,\n",
       " -0.011977162690798964,\n",
       " 0.037164890761317106,\n",
       " -0.013573700173267852,\n",
       " -0.025018683703243855,\n",
       " -0.007738511852989456,\n",
       " 0.019045754165658884,\n",
       " 0.028099061181568623,\n",
       " -0.001748365187110131,\n",
       " 0.0013594048175352544,\n",
       " 0.00297394267061505,\n",
       " 0.00012159903175714403,\n",
       " 0.010618539951427,\n",
       " 0.0014517536035716127,\n",
       " 0.018344529706239017,\n",
       " 0.009172264853119508,\n",
       " -0.01108184966774994,\n",
       " -0.03127961358266766,\n",
       " 0.005168399048769851,\n",
       " -0.005334313706407066,\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the length, it will be used for the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Pinecone\n",
    "\n",
    "On this stage I will load my document into Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'arxiv-summarizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.control.pinecone.Pinecone at 0x204a8d12490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.Pinecone(\n",
    "    api_key=pinecone_api_key,\n",
    "    enviornment=environment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Pinecone.from_documents(documents, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity for Data Querying\n",
    "\n",
    "In order to retrieve certain data from the query we will use cosine similarity for our Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_query(query, k=2):\n",
    "    matching_results = index.similarity_search(query, k=k)\n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\egora\\ML\\nlp_related\\article-summarizer\\venv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\egora\\ML\\nlp_related\\article-summarizer\\venv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.6, api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's retrieve queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_answers(query):\n",
    "    doc_search = retrieve_query(query)\n",
    "    print(doc_search)\n",
    "    response = chain.run(input_documents=doc_search, question=query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='LLMs are more friendly to CPU devices, which are the main processors used in edge and mobile\\ndevices. This means that BitNet b1.58 can be efficiently executed on these devices, further improving\\ntheir performance and capabilities.\\nNew Hardware for 1-bit LLMs\\nRecent work like Groq5 has demonstrated promising results and great potential for building specific\\nhardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design\\nnew hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm\\nenabled in BitNet [WMD+23].\\nReferences\\n[BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA:\\nreasoning about physical commonsense in natural language. CoRR, abs/1911.11641,\\n2019.', metadata={'Authors': 'Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei', 'Published': '2024-02-27', 'Summary': 'Recent research, such as BitNet, is paving the way for a new era of 1-bit\\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\\nTransformer LLM with the same model size and training tokens in terms of both\\nperplexity and end-task performance, while being significantly more\\ncost-effective in terms of latency, memory, throughput, and energy consumption.\\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\\ntraining new generations of LLMs that are both high-performance and\\ncost-effective. Furthermore, it enables a new computation paradigm and opens\\nthe door for designing specific hardware optimized for 1-bit LLMs.', 'Title': 'The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits'}), Document(page_content='LLMs are more friendly to CPU devices, which are the main processors used in edge and mobile\\ndevices. This means that BitNet b1.58 can be efficiently executed on these devices, further improving\\ntheir performance and capabilities.\\nNew Hardware for 1-bit LLMs\\nRecent work like Groq5 has demonstrated promising results and great potential for building specific\\nhardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design\\nnew hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm\\nenabled in BitNet [WMD+23].\\nReferences\\n[BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA:\\nreasoning about physical commonsense in natural language. CoRR, abs/1911.11641,\\n2019.', metadata={'Authors': 'Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei', 'Published': '2024-02-27', 'Summary': 'Recent research, such as BitNet, is paving the way for a new era of 1-bit\\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\\nTransformer LLM with the same model size and training tokens in terms of both\\nperplexity and end-task performance, while being significantly more\\ncost-effective in terms of latency, memory, throughput, and energy consumption.\\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\\ntraining new generations of LLMs that are both high-performance and\\ncost-effective. Furthermore, it enables a new computation paradigm and opens\\nthe door for designing specific hardware optimized for 1-bit LLMs.', 'Title': 'The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits'})]\n",
      "They reach the 1 bit through the new computation paradigm enabled in BitNet.\n"
     ]
    }
   ],
   "source": [
    "custom_query = 'How to they reach the 1 bit?'\n",
    "\n",
    "answer = retrieve_answers(custom_query)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "In this part of the notebook I will write code for text summarization for the same article. The method which will be used here is a MapReduce. What I will do is that the text is separated into chunks earlier. Each chunk is then sent over to the model. Model creates summary for every chunk. Then from those summaries the final summary gets created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveJsonSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\egora\\ML\\nlp_related\\article-summarizer\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_summary = ChatOpenAI(temperature=0.3, model_name=\"gpt-3.5-turbo-0125\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm_summary,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = chain.run(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces BitNet b1.58, a 1-bit Large Language Model that matches the performance of full-precision models while being more cost-effective in terms of latency, memory, throughput, and energy consumption. It introduces a new scaling law and training method for high-performance and cost-effective LLMs, allowing for improved efficiency in loading weights from DRAM. BitNet b1.58 outperforms existing models in terms of memory consumption, throughput, and latency, showing promise in reducing the cost of large language models while maintaining performance. The study compares BitNet b1.58 to LLaMA LLM models, demonstrating its superior performance in various tasks and its potential for addressing challenges related to memory consumption and energy efficiency. Further compression to 4 bits or lower is possible for future work, making 1.58-bit LLMs a cost-effective solution for edge and mobile devices.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use custom prompt for the MapReduce type of summarization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreduce_prompt = \"\"\"\n",
    "You are an expert in Data Science and Data Analytics. You can easilty understand Data Science scientific papers.\n",
    "Please summarize the following text:\n",
    "Text: `{documents}`\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_template = PromptTemplate(input_variables=['documents'],\n",
    "                                     template=mapreduce_prompt\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comb_prompt = \"\"\"\n",
    "You are an expert in Data Science and Data Analytics. You can easilty understand Data Science scientific papers.\n",
    "Now I want you to take a deep breath and provide a final summary of the entire text with these important points.\n",
    "Add a Generic Motivation Title.\n",
    "Start with comprehensive summary. Limit yourself with 250 word. In the end add key takeaways in up to 5 bullit points.\n",
    "Text: `{documents}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comb_prompt_template = PromptTemplate(input_variables=['documents'],\n",
    "                                            template=final_comb_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name text was not found in llm_chain input_variables: ['documents'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summary_chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(\n\u001b[0;32m      2\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm_summary,\n\u001b[0;32m      3\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap_reduce\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     map_prompt\u001b[38;5;241m=\u001b[39mmap_prompt_template,\n\u001b[0;32m      5\u001b[0m     combine_prompt\u001b[38;5;241m=\u001b[39mfinal_comb_prompt_template,\n\u001b[0;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\egora\\ML\\nlp_related\\article-summarizer\\venv\\Lib\\site-packages\\langchain\\chains\\summarize\\__init__.py:160\u001b[0m, in \u001b[0;36mload_summarize_chain\u001b[1;34m(llm, chain_type, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m     )\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loader_mapping[chain_type](llm, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\egora\\ML\\nlp_related\\article-summarizer\\venv\\Lib\\site-packages\\langchain\\chains\\summarize\\__init__.py:67\u001b[0m, in \u001b[0;36m_load_map_reduce_chain\u001b[1;34m(llm, map_prompt, combine_prompt, combine_document_variable_name, map_reduce_document_variable_name, collapse_prompt, reduce_llm, collapse_llm, verbose, token_max, callbacks, collapse_max_retries, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m reduce_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[0;32m     64\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_reduce_llm, prompt\u001b[38;5;241m=\u001b[39mcombine_prompt, verbose\u001b[38;5;241m=\u001b[39mverbose, callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[0;32m     68\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39mreduce_chain,\n\u001b[0;32m     69\u001b[0m     document_variable_name\u001b[38;5;241m=\u001b[39mcombine_document_variable_name,\n\u001b[0;32m     70\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m     71\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collapse_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     collapse_chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\egora\\ML\\nlp_related\\article-summarizer\\venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\egora\\ML\\nlp_related\\article-summarizer\\venv\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name text was not found in llm_chain input_variables: ['documents'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(\n",
    "    llm=llm_summary,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt_template,\n",
    "    combine_prompt=final_comb_prompt_template,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
